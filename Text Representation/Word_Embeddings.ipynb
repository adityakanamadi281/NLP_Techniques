{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0143a2-eefc-457f-a81d-4d143fd49735",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "* It represents words as numbers so computers can understand similarities:\n",
    "* Turns words into vectors\n",
    "* Similar words have similar vectors\n",
    "\n",
    "## What are the Embeddings?\n",
    "* Embeddings are the vector representation of categorical data\n",
    "* Computers can understand the meaning and relationships between them.\n",
    "\n",
    "## Why Do we Need Embeddings ?\n",
    "* In the past , methods like one-hot encoding were used to represent words.\n",
    "  * This approach has limitations:\n",
    "    * NO Seamntic Meaning - One hot vectors don't capture relationships between words.\n",
    "    * High-Dimensionality - for large vocabularies, these vectors become very sparse and memory-intensive\n",
    "\n",
    "## What can we Do with Embeddings ?\n",
    "* If you ask a computer to find words similar to \"king\", it will look at the numbers and see that \"queen\" is close to \"king\" because they both have Royalty score.\n",
    "* For sentences Embeddings can help computers to understand meaning, it can find relevant information even if the extra words don't match.\n",
    "* Embeddings allow computers to group based on meaning, not just exact words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4d03eb-95bb-406e-946b-c01317a82d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams: [('This',), ('is',), ('a',), ('sample',), ('text',), ('fpr',), ('n-grams',), ('implementation',), ('in',), ('NLP',)]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words=text.split()\n",
    "    ngrams=[tuple(words[i:i+1]) for i in range(len(words)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "dummy_text= \"This is a sample text fpr n-grams implementation in NLP using Python.\"\n",
    "\n",
    "trigrams=generate_ngrams(dummy_text, 3)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0263c0af-03f1-4249-8a18-a71a5cb634ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\adity\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install  gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e978a43-c36a-49d9-9f7e-6c61f3f51716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'sample' :  [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
      "Similar Words to 'sample' :  [('example', 0.17018887400627136), ('a', 0.13887979090213776), ('is', 0.034764934331178665)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dummy_corpus = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in dummy_corpus]\n",
    "\n",
    "# CBOW Model \n",
    "cbow_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=0, min_count=1)\n",
    "\n",
    "\n",
    "# test the CBOW Model \n",
    "word_vector=cbow_model.wv['sample']\n",
    "similar_words=cbow_model.wv.most_similar('sample', topn=3)\n",
    "\n",
    "print(\"Word Vector for 'sample' : \", word_vector)\n",
    "print(\"Similar Words to 'sample' : \", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ac20c-7ebb-4571-a062-30470954ef83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
