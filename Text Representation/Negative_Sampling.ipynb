{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cc83c1-18f6-445a-91ee-91239a30e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word (Word2Vec) :  is\n",
      "Next Word (Negative Sampling) :  is\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus=[\"Word embeddings are powerful.\", \"TThey capture semantic relationships.\", \"Word2Vec is a popular model.\"]\n",
    "\n",
    "tokenized_corpus=[word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1)\n",
    "\n",
    "neg_sampling_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, negative=5, min_count=1)\n",
    "\n",
    "word_to_predict = \"word\"\n",
    "next_word_w2v=w2v_model.wv.most_similar(word_to_predict, topn=1)[0][0]\n",
    "next_word_neg_sampling = neg_sampling_model.wv.most_similar(word_to_predict, topn=1)[0][0]\n",
    "\n",
    "print(\"Next Word (Word2Vec) : \", next_word_w2v)\n",
    "print(\"Next Word (Negative Sampling) : \", next_word_neg_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "859f3dfc-80c4-423e-89b6-397f728ee8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurances Matrix : \n",
      "[[0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]]\n",
      "Cosine Similarity Matrix : \n",
      "[[1.         0.36514837 0.         0.54772256 0.36514837 0.2236068\n",
      "  0.         0.31622777 0.36514837 0.2236068  0.54772256 0.2236068 ]\n",
      " [0.36514837 1.         0.81649658 0.33333333 0.         0.\n",
      "  0.33333333 0.57735027 0.         0.         0.33333333 0.20412415]\n",
      " [0.         0.81649658 1.         0.         0.         0.\n",
      "  0.40824829 0.35355339 0.         0.         0.         0.25      ]\n",
      " [0.54772256 0.33333333 0.         1.         0.33333333 0.81649658\n",
      "  0.         0.28867513 0.33333333 0.40824829 0.66666667 0.40824829]\n",
      " [0.36514837 0.         0.         0.33333333 1.         0.40824829\n",
      "  0.         0.         0.33333333 0.40824829 0.33333333 0.61237244]\n",
      " [0.2236068  0.         0.         0.81649658 0.40824829 1.\n",
      "  0.         0.         0.40824829 0.5        0.40824829 0.5       ]\n",
      " [0.         0.33333333 0.40824829 0.         0.         0.\n",
      "  1.         0.57735027 0.         0.         0.         0.40824829]\n",
      " [0.31622777 0.57735027 0.35355339 0.28867513 0.         0.\n",
      "  0.57735027 1.         0.         0.         0.28867513 0.1767767 ]\n",
      " [0.36514837 0.         0.         0.33333333 0.33333333 0.40824829\n",
      "  0.         0.         1.         0.40824829 0.33333333 0.61237244]\n",
      " [0.2236068  0.         0.         0.40824829 0.40824829 0.5\n",
      "  0.         0.         0.40824829 1.         0.81649658 0.5       ]\n",
      " [0.54772256 0.33333333 0.         0.66666667 0.33333333 0.40824829\n",
      "  0.         0.28867513 0.33333333 0.81649658 1.         0.40824829]\n",
      " [0.2236068  0.20412415 0.25       0.40824829 0.61237244 0.5\n",
      "  0.40824829 0.1767767  0.61237244 0.5        0.40824829 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "corpus = [\"Word embeddings are powerful.\", \"They capture semantic relationships.\", \"Count-based methods are efficient.\"]\n",
    "\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "co_occurance_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for i, target_word in enumerate(sentence):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sentence), i+window_size + 1)\n",
    "        context_words = [sentence[j] for j in range(start, end) if j != i ]\n",
    "        target_index = word_to_index[target_word]\n",
    "        for context_word in context_words:\n",
    "            context_index = word_to_index[context_word]\n",
    "            co_occurance_matrix[target_index, context_index] += 1\n",
    "\n",
    "print(\"Co-occurances Matrix : \")\n",
    "print(co_occurance_matrix)\n",
    "\n",
    "similarity_matrix = cosine_similarity(co_occurance_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix : \")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45603cb9-fb90-4538-a8f2-10887530fb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
